{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Installing and Using NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download() # download all packages/collections in nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Sampath, how are you doing today?', 'The weather is great and NLTK is interesting.', 'It is snowing here and really, it is pretty cold.', 'Though at -5 degrees, actually quite warmer than usual!', 'Okay, bye now.', 'ttyl.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "example_text = \"Hello Mr. Sampath, how are you doing today? The weather is great and NLTK is interesting. It is snowing here and really, it is pretty cold. Though at -5 degrees, actually quite warmer than usual! Okay, bye now. ttyl.\"\n",
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Sampath', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'NLTK', 'is', 'interesting', '.', 'It', 'is', 'snowing', 'here', 'and', 'really', ',', 'it', 'is', 'pretty', 'cold', '.', 'Though', 'at', '-5', 'degrees', ',', 'actually', 'quite', 'warmer', 'than', 'usual', '!', 'Okay', ',', 'bye', 'now', '.', 'ttyl', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Stop Words\n",
    "Stop words are words which are filtered out before or after processing of natural language data. Stop words usually refers to the most common words in a language.\n",
    "\n",
    "(Note: there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'sentence', 'showing', 'stop', 'word', 'filtration', '.', 'Hopefully', ',', 'work', '!', 'Or', ',', '?', 'Wont', '?', '?', 'Lets', 'find', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "example_sentence = \"This is an example sentence showing stop word filtration. Hopefully, this will work! Or, will it? Wont it?? Lets find out!!\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(example_sentence)\n",
    "filtered_sentence = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "# The above for loop as a one-liner\n",
    "# filtered_sentence = [w for w in words if w not in stop_words]\n",
    "\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Reducing different forms of a word to a common base form. For example: \n",
    "- “I am a student” = “I be a student”;  \n",
    "- “My dog’s fur is dark” = “My dog fur be dark”.\n",
    "\n",
    "Note: Stemming may not be needed aynmore with modern NLP tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n",
      "It\n",
      "is\n",
      "veri\n",
      "import\n",
      "to\n",
      "be\n",
      "pythonli\n",
      "while\n",
      "you\n",
      "are\n",
      "python\n",
      "with\n",
      "python\n",
      ".\n",
      "all\n",
      "python\n",
      "have\n",
      "python\n",
      "badli\n",
      "at\n",
      "least\n",
      "onc\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer # Using the Porter Stemming Algorithm from 1979!\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"python\",\"pythoning\",\"pythoner\",\"pythoned\",\"pythonly\"]\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))\n",
    "\n",
    "example_sentence = \"It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned badly at least once.\"\n",
    "words = word_tokenize(example_sentence)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
      "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
      "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
      "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n",
      "[('31', 'CD'), (',', ','), ('2006', 'CD'), ('.', '.')]\n",
      "[('White', 'NNP'), ('House', 'NNP'), ('photo', 'NN'), ('by', 'IN'), ('Eric', 'NNP'), ('DraperEvery', 'NNP'), ('time', 'NN'), ('I', 'PRP'), (\"'m\", 'VBP'), ('invited', 'JJ'), ('to', 'TO'), ('this', 'DT'), ('rostrum', 'NN'), (',', ','), ('I', 'PRP'), (\"'m\", 'VBP'), ('humbled', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('privilege', 'NN'), (',', ','), ('and', 'CC'), ('mindful', 'NN'), ('of', 'IN'), ('the', 'DT'), ('history', 'NN'), ('we', 'PRP'), (\"'ve\", 'VBP'), ('seen', 'VBN'), ('together', 'RB'), ('.', '.')]\n",
      "[('We', 'PRP'), ('have', 'VBP'), ('gathered', 'VBN'), ('under', 'IN'), ('this', 'DT'), ('Capitol', 'NNP'), ('dome', 'NN'), ('in', 'IN'), ('moments', 'NNS'), ('of', 'IN'), ('national', 'JJ'), ('mourning', 'NN'), ('and', 'CC'), ('national', 'JJ'), ('achievement', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('have', 'VBP'), ('served', 'VBN'), ('America', 'NNP'), ('through', 'IN'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('consequential', 'JJ'), ('periods', 'NNS'), ('of', 'IN'), ('our', 'PRP$'), ('history', 'NN'), ('--', ':'), ('and', 'CC'), ('it', 'PRP'), ('has', 'VBZ'), ('been', 'VBN'), ('my', 'PRP$'), ('honor', 'NN'), ('to', 'TO'), ('serve', 'VB'), ('with', 'IN'), ('you', 'PRP'), ('.', '.')]\n",
      "[('In', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('two', 'CD'), ('parties', 'NNS'), (',', ','), ('two', 'CD'), ('chambers', 'NNS'), (',', ','), ('and', 'CC'), ('two', 'CD'), ('elected', 'JJ'), ('branches', 'NNS'), (',', ','), ('there', 'EX'), ('will', 'MD'), ('always', 'RB'), ('be', 'VB'), ('differences', 'NNS'), ('and', 'CC'), ('debate', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer # Unsupervised tokenizer, can also be retrained if needed\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\") # train on this text\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[1:10]: # restricting to first 10 sentences\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            print(tagged)\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chunking\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
